{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "744fb778",
   "metadata": {},
   "source": [
    "## 微调模型之后的测试推理阶段\n",
    "1. 使用构造的数据集微调llama3-8b-instruct\n",
    "2. 可以进行单句的推理测试\n",
    "3. 模型和lora参数保存在jupyterlab中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "881562f9-e208-4b0f-ad6d-fcd0b32e7fc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T12:18:34.373075Z",
     "iopub.status.busy": "2024-12-13T12:18:34.372171Z",
     "iopub.status.idle": "2024-12-13T12:18:34.379892Z",
     "shell.execute_reply": "2024-12-13T12:18:34.378570Z",
     "shell.execute_reply.started": "2024-12-13T12:18:34.373011Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f501f142-7422-45f8-a9b0-252b758d02e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T12:18:37.782603Z",
     "iopub.status.busy": "2024-12-13T12:18:37.781686Z",
     "iopub.status.idle": "2024-12-13T12:18:40.279988Z",
     "shell.execute_reply": "2024-12-13T12:18:40.278550Z",
     "shell.execute_reply.started": "2024-12-13T12:18:37.782529Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     AutoConfig,\n\u001b[0;32m      5\u001b[0m     AutoTokenizer,\n\u001b[0;32m      6\u001b[0m     AutoModelForCausalLM,\n\u001b[0;32m      7\u001b[0m     GenerationConfig\n\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = 'model/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6'\n",
    "lora_path = 'lora/checkpoint-11630'\n",
    "config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aab488c4-9d47-48a1-afcb-c26d3a4e7ba7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T12:18:59.782566Z",
     "iopub.status.busy": "2024-12-13T12:18:59.781685Z",
     "iopub.status.idle": "2024-12-13T12:19:00.670680Z",
     "shell.execute_reply": "2024-12-13T12:19:00.669840Z",
     "shell.execute_reply.started": "2024-12-13T12:18:59.782490Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  6.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    config=config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    lora_path,\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91176087-84d1-4e61-8b2e-716faea0f4f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T12:19:10.782068Z",
     "iopub.status.busy": "2024-12-13T12:19:10.781154Z",
     "iopub.status.idle": "2024-12-13T12:19:10.792389Z",
     "shell.execute_reply": "2024-12-13T12:19:10.791387Z",
     "shell.execute_reply.started": "2024-12-13T12:19:10.782003Z"
    }
   },
   "outputs": [],
   "source": [
    "def infer(text_string):\n",
    "    system_prompt = '<<SYS>>\\n 你是一个乐于助人的助手。\\n<</SYS>>\\n\\n'\n",
    "    input_data = {\n",
    "        \"instruction\": \"你是一个安全事故领域专家。请根据input的问题作出回答。\",\n",
    "        \"input\": text_string\n",
    "    }\n",
    "    sintruct = json.dumps(input_data, ensure_ascii=False)\n",
    "    sintruct = '[INST]' + system_prompt + sintruct + '[/INST]\\n'\n",
    "    # sintruct = '<reserved_106>' + system_prompt + sintruct + '<reserved_107>'\n",
    "\n",
    "    input_ids = tokenizer.encode(\n",
    "        sintruct,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    input_length = input_ids.size(1)\n",
    "    print(\"Token数量:\", input_length)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=GenerationConfig(\n",
    "                max_length=2048,\n",
    "                max_new_tokens=512,\n",
    "                return_dict_in_generate=True,\n",
    "                # do_sample=True,\n",
    "                # temperature=1\n",
    "            ),\n",
    "            pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    generation_output = generation_output.sequences[0]\n",
    "    generation_output = generation_output[input_length:]\n",
    "    output = tokenizer.decode(generation_output, skip_special_tokens=True)\n",
    "    print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "278f3d50-a15e-4ffa-9565-2b5c8246430d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T12:22:36.142828Z",
     "iopub.status.busy": "2024-12-13T12:22:36.141936Z",
     "iopub.status.idle": "2024-12-13T12:22:55.632321Z",
     "shell.execute_reply": "2024-12-13T12:22:55.631143Z",
     "shell.execute_reply.started": "2024-12-13T12:22:36.142748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token数量: 56\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# infer(\"在某次海上搜救任务中，搜救中心收到了一条紧急信息，一艘载有200人的客轮在远洋航行中遇到了暴风雨，导致船体严重受损，水进入船舱，船上的人员处于极度危险之中。客轮目前已失去动力，随时可能沉没。考虑到此次海上突发事件对人命安全的严重威胁，搜救中心需要判断并上报此事件为最高级别。此次事件应归类为哪个险情等级？\")\n",
    "infer(\"Please answer what is one plus one?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
