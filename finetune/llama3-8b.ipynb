{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b56f102-a00a-4277-aa76-99e2237611ac",
   "metadata": {},
   "source": [
    "## instructionè¯­æ–™å¾®è°ƒllama3-8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a5cf039-4b2c-431b-802f-949b075138dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T06:50:31.233788Z",
     "iopub.status.busy": "2024-12-13T06:50:31.233572Z",
     "iopub.status.idle": "2024-12-13T06:50:31.240860Z",
     "shell.execute_reply": "2024-12-13T06:50:31.240258Z",
     "shell.execute_reply.started": "2024-12-13T06:50:31.233766Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718cd569-4025-47a1-b6cf-2c10c71af5c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ä¹‹å‰çš„å¾®è°ƒä»£ç "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ed66cb-e56f-46ce-a291-b86159ad01d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import _utils\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = 'meta-llama/Meta-Llama-3-8B'\n",
    "lora_path = 'lora/llama3-8B-iepile-data2text-continue'\n",
    "config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206b3ad3-a60c-466b-9aec-d94eebee9c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    config=config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3509645e-c2f3-4a97-b43a-b9e4803b729e",
   "metadata": {},
   "source": [
    "### ç°åœ¨ä½¿ç”¨çš„å¾®è°ƒä»£ç "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b29cf09a-5b25-4ee5-a1d6-2d7529c99068",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T06:50:33.886417Z",
     "iopub.status.busy": "2024-12-13T06:50:33.885185Z",
     "iopub.status.idle": "2024-12-13T06:50:41.673520Z",
     "shell.execute_reply": "2024-12-13T06:50:41.672499Z",
     "shell.execute_reply.started": "2024-12-13T06:50:33.886361Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import _utils\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "# åŠ è½½Llamaæ¨¡å‹å’ŒTokenizer\n",
    "model_path = \"../model/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6\"\n",
    "config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    config=config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# é…ç½®LoRAå¾®è°ƒ\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # LoRAå‚æ•°ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´\n",
    "    lora_alpha=16,  # LoRAå‚æ•°ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´\n",
    "    lora_dropout=0.05,  # LoRAå‚æ•°ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´\n",
    "    bias=\"none\",  # LoRAå¾®è°ƒçš„åç½®é€‰é¡¹\n",
    "    task_type=\"CAUSAL_LM\"  # ä»»åŠ¡ç±»å‹ï¼Œé€šå¸¸ä½¿ç”¨CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7623782-2584-432f-a9f5-6d485d318e69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T06:50:45.225873Z",
     "iopub.status.busy": "2024-12-13T06:50:45.224312Z",
     "iopub.status.idle": "2024-12-13T06:50:47.122957Z",
     "shell.execute_reply": "2024-12-13T06:50:47.121888Z",
     "shell.execute_reply.started": "2024-12-13T06:50:45.225807Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "with open('../ft_data/train_data_v1.json', 'r', encoding='utf-8') as f:\n",
    "    train_data_raw = json.load(f)\n",
    "\n",
    "with open('../ft_data/val_data_v1.json', 'r', encoding='utf-8') as f:\n",
    "    val_data_raw = json.load(f)\n",
    "\n",
    "\n",
    "# æ•°æ®é¢„å¤„ç†ï¼šå°† Instruction å’Œ Input åˆå¹¶ä¸ºè¾“å…¥ï¼ŒResponse ä½œä¸ºè¾“å‡º\n",
    "def preprocess_data(data, tokenizer, max_length=512):\n",
    "    instructions = [item[\"Instruction\"] for item in data]\n",
    "    inputs = [item[\"Input\"] for item in data]\n",
    "    responses = [item[\"Response\"] for item in data]\n",
    "\n",
    "    # åˆå¹¶ Instruction å’Œ Input\n",
    "    inputs_combined = [f\"{instruction}\\n{input_text}\" for instruction, input_text in zip(instructions, inputs)]\n",
    "\n",
    "    # ä½¿ç”¨ç»Ÿä¸€çš„max_lengthå¡«å……ï¼Œå¹¶ç¡®ä¿inputså’Œlabelsé•¿åº¦ä¸€è‡´\n",
    "    model_inputs = tokenizer(inputs_combined, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    labels = tokenizer(responses, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "    # ç¡®ä¿ input_ids å’Œ labels é•¿åº¦ä¸€è‡´\n",
    "    assert model_inputs[\"input_ids\"].shape[1] == labels.shape[1], f\"Length mismatch: {model_inputs['input_ids'].shape[1]} != {labels.shape[1]}\"\n",
    "\n",
    "    return {\"input_ids\": model_inputs[\"input_ids\"], \"labels\": labels}\n",
    "\n",
    "\n",
    "# å°†æ•°æ®è½¬æ¢ä¸ºåˆé€‚çš„æ ¼å¼\n",
    "train_data_0 = [{\"Instruction\": item[\"instruction\"], \"Response\": item[\"output\"], \"Input\": item[\"rule\"]} for item in train_data_raw]\n",
    "val_data_0 = [{\"Instruction\": item[\"instruction\"], \"Response\": item[\"output\"], \"Input\": item[\"rule\"]} for item in val_data_raw]\n",
    "\n",
    "# å°†æ•°æ®è½¬æ¢ä¸ºåˆé€‚çš„æ ¼å¼\n",
    "train_data = preprocess_data(train_data_0, tokenizer)\n",
    "val_data = preprocess_data(val_data_0, tokenizer)\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "val_dataset = Dataset.from_dict(val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85cd3d5a-9d50-42ef-a74b-f809cbb2fd62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T06:50:51.424103Z",
     "iopub.status.busy": "2024-12-13T06:50:51.422563Z",
     "iopub.status.idle": "2024-12-13T09:06:24.176848Z",
     "shell.execute_reply": "2024-12-13T09:06:24.175079Z",
     "shell.execute_reply.started": "2024-12-13T06:50:51.424036Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_8053/3481787107.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11630' max='11630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11630/11630 2:15:31, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.613200</td>\n",
       "      <td>0.607305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.684600</td>\n",
       "      <td>0.593674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.742600</td>\n",
       "      <td>0.567335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.842900</td>\n",
       "      <td>0.576224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.562700</td>\n",
       "      <td>0.567671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.916300</td>\n",
       "      <td>0.590891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.563700</td>\n",
       "      <td>0.596283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.521800</td>\n",
       "      <td>0.629527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.661500</td>\n",
       "      <td>0.619426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.312100</td>\n",
       "      <td>0.640178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11630, training_loss=0.5403298856672909, metrics={'train_runtime': 8132.4351, 'train_samples_per_second': 2.859, 'train_steps_per_second': 1.43, 'total_flos': 5.36275143622656e+17, 'train_loss': 0.5403298856672909, 'epoch': 10.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../lora\",  # è¾“å‡ºç›®å½•\n",
    "    num_train_epochs=10,  # è®­ç»ƒè½®æ•°\n",
    "    per_device_train_batch_size=2,  # æ¯ä¸ªè®¾å¤‡çš„è®­ç»ƒæ‰¹é‡å¤§å°\n",
    "    per_device_eval_batch_size=2,  # æ¯ä¸ªè®¾å¤‡çš„è¯„ä¼°æ‰¹é‡å¤§å°\n",
    "    evaluation_strategy=\"epoch\",  # æ¯ä¸ªepochè¿›è¡Œè¯„ä¼°\n",
    "    save_strategy=\"epoch\",  # æ¯ä¸ªepochä¿å­˜æ¨¡å‹\n",
    "    logging_dir=\"../logs\",  # æ—¥å¿—ç›®å½•\n",
    "    logging_steps=10,  # æ¯10æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—\n",
    "    save_steps=500,  # æ¯500æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹\n",
    "    eval_steps=500,  # æ¯500æ­¥è¯„ä¼°ä¸€æ¬¡æ¨¡å‹\n",
    "    load_best_model_at_end=True,  # åœ¨è®­ç»ƒç»“æŸæ—¶åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "    # metric_for_best_model=\"accuracy\",  # æ ¹æ®å‡†ç¡®ç‡é€‰æ‹©æœ€ä½³æ¨¡å‹\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dcdbc59-ebf0-449d-a59d-cedee4ee64ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T09:22:49.067201Z",
     "iopub.status.busy": "2024-12-13T09:22:49.066380Z",
     "iopub.status.idle": "2024-12-13T09:22:49.177375Z",
     "shell.execute_reply": "2024-12-13T09:22:49.176563Z",
     "shell.execute_reply.started": "2024-12-13T09:22:49.067146Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next is the answer:\n",
      " åœ¨æŸæ¬¡æµ·ä¸Šæœæ•‘ä»»åŠ¡ä¸­ï¼Œæœæ•‘ä¸­å¿ƒæ”¶åˆ°äº†ä¸€æ¡ç´§æ€¥ä¿¡æ¯ï¼Œä¸€è‰˜è½½æœ‰200äººçš„å®¢è½®åœ¨è¿œæ´‹èˆªè¡Œä¸­é‡åˆ°äº†æš´é£é›¨ï¼Œå¯¼è‡´èˆ¹ä½“ä¸¥é‡å—æŸï¼Œæ°´è¿›å…¥èˆ¹èˆ±ï¼Œèˆ¹ä¸Šçš„äººå‘˜å¤„äºæåº¦å±é™©ä¹‹ä¸­ã€‚å®¢è½®ç›®å‰å·²å¤±å»åŠ¨åŠ›ï¼Œéšæ—¶å¯èƒ½æ²‰æ²¡ã€‚è€ƒè™‘åˆ°æ­¤æ¬¡æµ·ä¸Šçªå‘äº‹ä»¶å¯¹äººå‘½å®‰å…¨çš„ä¸¥é‡å¨èƒï¼Œæœæ•‘ä¸­å¿ƒéœ€è¦åˆ¤æ–­å¹¶ä¸ŠæŠ¥æ­¤äº‹ä»¶ä¸ºæœ€é«˜çº§åˆ«ã€‚æ­¤æ¬¡äº‹ä»¶åº”å½’ç±»ä¸ºå“ªä¸ªé™©æƒ…ç­‰çº§ï¼Ÿ\n",
      "æµ·ä¸Šçªå‘äº‹ä»¶é™©æƒ…åˆ†çº§ï¼šæ ¹æ®å›½å®¶çªå‘äº‹ä»¶é™©æƒ…ä¸ŠæŠ¥çš„æœ‰å…³è§„å®šï¼Œå¹¶ç»“åˆæµ·ä¸Šçªå‘äº‹ä»¶çš„ç‰¹ç‚¹åŠçªå‘äº‹ä»¶å¯¹äººå‘½å®‰å…¨ã€æµ·æ´‹ç¯å¢ƒçš„å±å®³ç¨‹åº¦å’Œäº‹æ€å‘å±•è¶‹åŠ¿ï¼Œå°†æµ·ä¸Šçªå‘äº‹ä»¶é™©æƒ…ä¿¡æ¯åˆ†ä¸ºç‰¹å¤§ã€é‡å¤§ã€è¾ƒå¤§ã€ä¸€èˆ¬å››çº§ã€‚\n"
     ]
    }
   ],
   "source": [
    "# è¿›è¡Œæ¨ç†\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "inputs = tokenizer(\"åœ¨æŸæ¬¡æµ·ä¸Šæœæ•‘ä»»åŠ¡ä¸­ï¼Œæœæ•‘ä¸­å¿ƒæ”¶åˆ°äº†ä¸€æ¡ç´§æ€¥ä¿¡æ¯ï¼Œä¸€è‰˜è½½æœ‰200äººçš„å®¢è½®åœ¨è¿œæ´‹èˆªè¡Œä¸­é‡åˆ°äº†æš´é£é›¨ï¼Œå¯¼è‡´èˆ¹ä½“ä¸¥é‡å—æŸï¼Œæ°´è¿›å…¥èˆ¹èˆ±ï¼Œèˆ¹ä¸Šçš„äººå‘˜å¤„äºæåº¦å±é™©ä¹‹ä¸­ã€‚å®¢è½®ç›®å‰å·²å¤±å»åŠ¨åŠ›ï¼Œéšæ—¶å¯èƒ½æ²‰æ²¡ã€‚è€ƒè™‘åˆ°æ­¤æ¬¡æµ·ä¸Šçªå‘äº‹ä»¶å¯¹äººå‘½å®‰å…¨çš„ä¸¥é‡å¨èƒï¼Œæœæ•‘ä¸­å¿ƒéœ€è¦åˆ¤æ–­å¹¶ä¸ŠæŠ¥æ­¤äº‹ä»¶ä¸ºæœ€é«˜çº§åˆ«ã€‚æ­¤æ¬¡äº‹ä»¶åº”å½’ç±»ä¸ºå“ªä¸ªé™©æƒ…ç­‰çº§ï¼Ÿ\\næµ·ä¸Šçªå‘äº‹ä»¶é™©æƒ…åˆ†çº§ï¼šæ ¹æ®å›½å®¶çªå‘äº‹ä»¶é™©æƒ…ä¸ŠæŠ¥çš„æœ‰å…³è§„å®šï¼Œå¹¶ç»“åˆæµ·ä¸Šçªå‘äº‹ä»¶çš„ç‰¹ç‚¹åŠçªå‘äº‹ä»¶å¯¹äººå‘½å®‰å…¨ã€æµ·æ´‹ç¯å¢ƒçš„å±å®³ç¨‹åº¦å’Œäº‹æ€å‘å±•è¶‹åŠ¿ï¼Œå°†æµ·ä¸Šçªå‘äº‹ä»¶é™©æƒ…ä¿¡æ¯åˆ†ä¸ºç‰¹å¤§ã€é‡å¤§ã€è¾ƒå¤§ã€ä¸€èˆ¬å››çº§ã€‚\", return_tensors=\"pt\").to(device)\n",
    "# inputs = tokenizer(\"Translate the following sentence to French.\\nGood evening!\", return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=512)\n",
    "print('Next is the answer:\\n', tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
